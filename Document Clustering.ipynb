{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering\n",
    "\n",
    "In this notebook I attempt to cluster a set of documents posted on newsgroups using simple clustering mechanisms and feature extraction. The corpus is made up of 300 documents, each of which is characterised at least by some body text, and the newsgroups which it was posted in.\n",
    "\n",
    "## Prelim: Importing libraries\n",
    "\n",
    "First, run the below code to import. Only uncomment the BERT serving client if you want to run my BERT code (shown in Appendix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import pprint\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "# from bert_serving.client import BertClient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Extracting features\n",
    "\n",
    "I extracted features in 3 ways. The most effective method was actually the simplest, that being completely ignoring the body text of the document and encoding only the newsgroup categories of the document. For later topic analysis, however, I use the body text, and for efficiency I extract it here. This is done in the below code:\n",
    "\n",
    "### Extra:\n",
    "\n",
    "If we want to weight the newsgroup counts to have different degrees of importance (based on how often they appear in documents) we can use tf-idf. I build dictionaries for this below, though I don't use them later in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionaries\n",
    "\n",
    "tf = {}\n",
    "df = {}\n",
    "cleaned_dict = {}\n",
    "cleaned_sentences = {}\n",
    "email_ng = {}\n",
    "ng_set_total = set()\n",
    "\n",
    "# get files\n",
    "\n",
    "files = glob.glob('../data/*')\n",
    "# define function for removing punctuation from body text\n",
    "\n",
    "def clean_text(original_text):\n",
    "    cleaned = re.sub('[!/\\'\\\"(),.:#>-@*\\n]',' ', original_text)\n",
    "    return cleaned\n",
    "\n",
    "for file in files:\n",
    "    tf[file] = {}\n",
    "    with open(file,'r', encoding=\"utf8\", errors='ignore') as input:\n",
    "\n",
    "        text = input.read()\n",
    "\n",
    "        # get the newsgroup words and process them\n",
    "\n",
    "        newsgroup_match = re.search('Newsgroups: (.+)', text).group(1)\n",
    "        newsgroups = newsgroup_match.split(',')\n",
    "        ng_words = []\n",
    "        \n",
    "        for group in newsgroups:\n",
    "            subgroups = group.split('.')\n",
    "            ng_words.extend(subgroups)\n",
    "        \n",
    "        # some newsgroups e.g. 'talk' are repeated often -- we only need one of each tag\n",
    "        # therefore make it a set.\n",
    "        \n",
    "        ng_set = list(set(ng_words))\n",
    "        email_ng[file] = ng_set\n",
    "        for element in ng_set:\n",
    "            ng_set_total.add(element)\n",
    " \n",
    "        # now extract and clean body text\n",
    "            \n",
    "        body_text = re.search(r'\\n\\n((.|\\n)+)$', text).group()\n",
    "        cleaned = clean_text(body_text) # + ' ' + subject_match\n",
    "        cleaned_sentences[file] = cleaned.lower()\n",
    "        cleaned_sentences_list = cleaned.lower()\n",
    "        \n",
    "        for ng in ng_set:\n",
    "            try:\n",
    "                tf[file][ng] += 1\n",
    "            except KeyError:\n",
    "                tf[file][ng] = 1\n",
    "\n",
    "        for ng in ng_set_total:\n",
    "            try:\n",
    "                df[ng] += 1\n",
    "            except KeyError:\n",
    "                df[ng] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build a one-hot encoding for each document representing the newsgroups of each doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "new_files = []\n",
    "\n",
    "# give each newsgroup a numerical value\n",
    "\n",
    "ng2idx = {ng:idx for idx, ng in enumerate(ng_set_total)}\n",
    "total_ngs = len(ng_set_total)\n",
    "\n",
    "# encode each file\n",
    "\n",
    "for file, ngs in email_ng.items():\n",
    "    encoding = np.zeros(total_ngs)\n",
    "    for ng in ngs:\n",
    "        encoding[ng2idx[ng]] = 1\n",
    "        # comment out above and uncomment below to give sentence vectors tf-idf weighting.\n",
    "        # encoding[ng2idx[ng]] = math.log(tf[file][ng]+1,10)*math.log(300/df[ng],10)\n",
    "    points.append(encoding)\n",
    "    new_files.append(file)\n",
    "    \n",
    "files = new_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Clustering the documents\n",
    "\n",
    "In this section we take the one-hot encoded documents and cluster them. This is done using a simple implementation of mean-shift clustering, in which we hill-climb to the highest density  point in n-dimensional space from each encoded document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the encodings -- this will be the initial 'mean' for each of the original points\n",
    "\n",
    "points_copy = points.copy()\n",
    "dimension = 120\n",
    "\n",
    "# list of final cluster centres\n",
    "\n",
    "cluster_centres = []\n",
    "\n",
    "for point in points_copy:\n",
    "    is_stable = False\n",
    "    # initialise theorised cluster point at an original encoding\n",
    "    current_point = point\n",
    "    while is_stable == False:\n",
    "        update = np.zeros(dimension)\n",
    "        n = 0\n",
    "        # search throughout all original points\n",
    "        for original_point in points:\n",
    "            # if original point is within a certain distance of the theorised cluster point\n",
    "            # then include in update.\n",
    "            if np.linalg.norm(current_point - original_point) < 2.25:\n",
    "                update += original_point\n",
    "                n += 1\n",
    "        # mean of all included points is the new theorised cluster point\n",
    "        new_point = update/n\n",
    "        # stop iterating if new theorised cluster point is identical to the current theorised cluster point\n",
    "        if (new_point == current_point).all():\n",
    "            is_stable = True\n",
    "        current_point = new_point\n",
    "    cluster_centres.append(current_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This now gives us a the centre of the cluster to which each encoded document belongs. Note that there is one parameter that must be chosen in this process -- the distance parameter. This parameter decides which original points are included in the recalculation of the cluster centre. I found an ideal value through trial and error.\n",
    "\n",
    "Now that we have a cluster centre for each document in the corpus, we can assign each document to a cluster. All documents with the same cluster centre can be said to come from the same cluster. This is done in the below code.\n",
    "\n",
    "Printed below are the 'clusters', and the files that comprise them. A cluster here must be comprised of at least one datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cluster_1': ['../data/38402',\n",
      "               '../data/38606',\n",
      "               '../data/38853',\n",
      "               '../data/38699',\n",
      "               '../data/38264',\n",
      "               '../data/38636',\n",
      "               '../data/38459',\n",
      "               '../data/38466',\n",
      "               '../data/38839',\n",
      "               '../data/38380',\n",
      "               '../data/38942',\n",
      "               '../data/38788',\n",
      "               '../data/38571',\n",
      "               '../data/38929',\n",
      "               '../data/38386',\n",
      "               '../data/38375',\n",
      "               '../data/38921',\n",
      "               '../data/39663',\n",
      "               '../data/39664',\n",
      "               '../data/37942',\n",
      "               '../data/39017',\n",
      "               '../data/38758',\n",
      "               '../data/38904',\n",
      "               '../data/39072',\n",
      "               '../data/38968',\n",
      "               '../data/39615',\n",
      "               '../data/37916',\n",
      "               '../data/39027',\n",
      "               '../data/38750',\n",
      "               '../data/38562',\n",
      "               '../data/37944',\n",
      "               '../data/38761',\n",
      "               '../data/38369',\n",
      "               '../data/38704',\n",
      "               '../data/37921',\n",
      "               '../data/38224',\n",
      "               '../data/38846',\n",
      "               '../data/38421',\n",
      "               '../data/38613',\n",
      "               '../data/38622',\n",
      "               '../data/38884',\n",
      "               '../data/38625',\n",
      "               '../data/38473',\n",
      "               '../data/38683',\n",
      "               '../data/38670',\n",
      "               '../data/38489',\n",
      "               '../data/38271',\n",
      "               '../data/38464',\n",
      "               '../data/38693',\n",
      "               '../data/38835',\n",
      "               '../data/38439',\n",
      "               '../data/38266',\n",
      "               '../data/38454',\n",
      "               '../data/38491',\n",
      "               '../data/38603',\n",
      "               '../data/38856',\n",
      "               '../data/38409',\n",
      "               '../data/38893',\n",
      "               '../data/38867',\n",
      "               '../data/38251',\n",
      "               '../data/39659',\n",
      "               '../data/39668',\n",
      "               '../data/38379',\n",
      "               '../data/38983',\n",
      "               '../data/37936',\n",
      "               '../data/38370',\n",
      "               '../data/38925',\n",
      "               '../data/39000',\n",
      "               '../data/38573',\n",
      "               '../data/37930',\n",
      "               '../data/38518',\n",
      "               '../data/39049',\n",
      "               '../data/38998',\n",
      "               '../data/38965',\n",
      "               '../data/39013',\n",
      "               '../data/38936',\n",
      "               '../data/38755',\n",
      "               '../data/39022',\n",
      "               '../data/38907',\n",
      "               '../data/38355',\n",
      "               '../data/38709',\n",
      "               '../data/38700',\n",
      "               '../data/39048',\n",
      "               '../data/39621',\n",
      "               '../data/39675',\n",
      "               '../data/37947',\n",
      "               '../data/38628',\n",
      "               '../data/38845',\n",
      "               '../data/38880',\n",
      "               '../data/38220',\n",
      "               '../data/38244',\n",
      "               '../data/38843',\n",
      "               '../data/38470',\n",
      "               '../data/38674'],\n",
      " 'cluster_10': ['../data/176986'],\n",
      " 'cluster_2': ['../data/53302',\n",
      "               '../data/54312',\n",
      "               '../data/54586',\n",
      "               '../data/54323',\n",
      "               '../data/54714',\n",
      "               '../data/53358',\n",
      "               '../data/55260',\n",
      "               '../data/53369',\n",
      "               '../data/55036',\n",
      "               '../data/54322',\n",
      "               '../data/54117',\n",
      "               '../data/53304',\n",
      "               '../data/54715',\n",
      "               '../data/54518',\n",
      "               '../data/54660',\n",
      "               '../data/54452',\n",
      "               '../data/54633',\n",
      "               '../data/54634',\n",
      "               '../data/54861',\n",
      "               '../data/55116',\n",
      "               '../data/54860',\n",
      "               '../data/54626',\n",
      "               '../data/54211',\n",
      "               '../data/54675',\n",
      "               '../data/54447',\n",
      "               '../data/54843',\n",
      "               '../data/54616',\n",
      "               '../data/54611',\n",
      "               '../data/54243',\n",
      "               '../data/54875',\n",
      "               '../data/54446',\n",
      "               '../data/54479',\n",
      "               '../data/54560',\n",
      "               '../data/55484',\n",
      "               '../data/54535',\n",
      "               '../data/54169',\n",
      "               '../data/53373',\n",
      "               '../data/53329',\n",
      "               '../data/54592',\n",
      "               '../data/54728',\n",
      "               '../data/54342',\n",
      "               '../data/54726',\n",
      "               '../data/55264',\n",
      "               '../data/55239',\n",
      "               '../data/55468',\n",
      "               '../data/54570',\n",
      "               '../data/55231',\n",
      "               '../data/54433',\n",
      "               '../data/54637',\n",
      "               '../data/54402',\n",
      "               '../data/54630',\n",
      "               '../data/54200',\n",
      "               '../data/54238',\n",
      "               '../data/54404',\n",
      "               '../data/54297',\n",
      "               '../data/54698',\n",
      "               '../data/55123',\n",
      "               '../data/54230',\n",
      "               '../data/54239',\n",
      "               '../data/54450',\n",
      "               '../data/55115',\n",
      "               '../data/55106',\n",
      "               '../data/54417',\n",
      "               '../data/54279',\n",
      "               '../data/54877',\n",
      "               '../data/54248',\n",
      "               '../data/178678',\n",
      "               '../data/54276',\n",
      "               '../data/54429',\n",
      "               '../data/54624',\n",
      "               '../data/54416',\n",
      "               '../data/53325',\n",
      "               '../data/54590',\n",
      "               '../data/55489',\n",
      "               '../data/54138',\n",
      "               '../data/54303',\n",
      "               '../data/55249',\n",
      "               '../data/54196',\n",
      "               '../data/54357',\n",
      "               '../data/55278',\n",
      "               '../data/54395',\n",
      "               '../data/54154',\n",
      "               '../data/54302',\n",
      "               '../data/54956',\n",
      "               '../data/55073',\n",
      "               '../data/53348',\n",
      "               '../data/55080',\n",
      "               '../data/54152',\n",
      "               '../data/54164'],\n",
      " 'cluster_3': ['../data/178527',\n",
      "               '../data/178349',\n",
      "               '../data/178382',\n",
      "               '../data/55063',\n",
      "               '../data/179097',\n",
      "               '../data/54748',\n",
      "               '../data/178341',\n",
      "               '../data/178517',\n",
      "               '../data/176881',\n",
      "               '../data/178924',\n",
      "               '../data/176886',\n",
      "               '../data/178851',\n",
      "               '../data/178668',\n",
      "               '../data/178661',\n",
      "               '../data/54659',\n",
      "               '../data/176930',\n",
      "               '../data/178455',\n",
      "               '../data/178656',\n",
      "               '../data/176984',\n",
      "               '../data/176983',\n",
      "               '../data/177008',\n",
      "               '../data/54449',\n",
      "               '../data/176982',\n",
      "               '../data/178447',\n",
      "               '../data/178887',\n",
      "               '../data/178610',\n",
      "               '../data/176895',\n",
      "               '../data/178301',\n",
      "               '../data/178765',\n",
      "               '../data/178566',\n",
      "               '../data/178906',\n",
      "               '../data/178939',\n",
      "               '../data/178337',\n",
      "               '../data/178738',\n",
      "               '../data/178532',\n",
      "               '../data/179070',\n",
      "               '../data/178390',\n",
      "               '../data/178997',\n",
      "               '../data/55470',\n",
      "               '../data/178731',\n",
      "               '../data/53328',\n",
      "               '../data/178907',\n",
      "               '../data/178309',\n",
      "               '../data/176869',\n",
      "               '../data/178560',\n",
      "               '../data/178569',\n",
      "               '../data/178556',\n",
      "               '../data/178965',\n",
      "               '../data/178998',\n",
      "               '../data/178789',\n",
      "               '../data/178745',\n",
      "               '../data/55060',\n",
      "               '../data/176884',\n",
      "               '../data/178579',\n",
      "               '../data/178718',\n",
      "               '../data/179095',\n",
      "               '../data/179066',\n",
      "               '../data/178775',\n",
      "               '../data/55068',\n",
      "               '../data/178927',\n",
      "               '../data/178327',\n",
      "               '../data/178571',\n",
      "               '../data/178318',\n",
      "               '../data/176878',\n",
      "               '../data/178788',\n",
      "               '../data/179067',\n",
      "               '../data/178945',\n",
      "               '../data/178522',\n",
      "               '../data/178721',\n",
      "               '../data/178654',\n",
      "               '../data/178837',\n",
      "               '../data/54469',\n",
      "               '../data/178631',\n",
      "               '../data/54697',\n",
      "               '../data/176904',\n",
      "               '../data/178451',\n",
      "               '../data/176951',\n",
      "               '../data/176956',\n",
      "               '../data/178699',\n",
      "               '../data/178606',\n",
      "               '../data/178433',\n",
      "               '../data/178862',\n",
      "               '../data/178865',\n",
      "               '../data/176916',\n",
      "               '../data/178489',\n",
      "               '../data/178487',\n",
      "               '../data/178870',\n",
      "               '../data/178622',\n",
      "               '../data/176926',\n",
      "               '../data/176988',\n",
      "               '../data/178682',\n",
      "               '../data/178360',\n",
      "               '../data/178993',\n",
      "               '../data/178960',\n",
      "               '../data/178994',\n",
      "               '../data/178792',\n",
      "               '../data/179018',\n",
      "               '../data/178368',\n",
      "               '../data/178361',\n",
      "               '../data/54591',\n",
      "               '../data/178769',\n",
      "               '../data/178564',\n",
      "               '../data/178751',\n",
      "               '../data/178793',\n",
      "               '../data/54538'],\n",
      " 'cluster_4': ['../data/38980', '../data/39008', '../data/39078'],\n",
      " 'cluster_5': ['../data/38577'],\n",
      " 'cluster_6': ['../data/178799'],\n",
      " 'cluster_7': ['../data/178801',\n",
      "               '../data/178813',\n",
      "               '../data/178481',\n",
      "               '../data/178824'],\n",
      " 'cluster_8': ['../data/39620'],\n",
      " 'cluster_9': ['../data/38753']}\n"
     ]
    }
   ],
   "source": [
    "cluster_bank = []\n",
    "cluster_names = []\n",
    "cluster_to_file = {}\n",
    "cluster_counter = 0\n",
    "\n",
    "for i, centroid in enumerate(cluster_centres):\n",
    "    # check if the cluster centre doesn't already exist in the cluster bank\n",
    "    if not any((centroid == x).all() for x in cluster_bank):\n",
    "        cluster_counter += 1\n",
    "        cluster_name = 'cluster_'+str(cluster_counter)\n",
    "        cluster_bank.append(centroid)\n",
    "        cluster_names.append(cluster_name)\n",
    "        cluster_to_file[cluster_name] = [files[i]]\n",
    "    # if cluster centre is in bank, get cluster name and assign it to the document\n",
    "    else:\n",
    "        lst = [(centroid == x).all() for x in cluster_bank]\n",
    "        idx = lst.index(True)\n",
    "        cluster_name = cluster_names[idx]\n",
    "        cluster_to_file[cluster_name].append(files[i])\n",
    "        \n",
    "pprint.pprint(cluster_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "\n",
    "My clustering algorithm reports three major document clusters. There are other, small, clusters, although they are of such low document count it seems more justifiable to treat them as outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Topic analysis of each cluster\n",
    "\n",
    "### Normalisation\n",
    "\n",
    "Now that we have clusters, we can find the topic of each cluster. I use Latent Dirichlet Allocation to perform topic modelling on the body text of each cluster. This requires building a bag of words model to represent each document in the corpus. To make this easier, we stemmatise and lemmatise all words. The functions for preprocessing the body text is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def process_tokens(token):\n",
    "    if len(token) > 3 and token not in STOPWORDS:\n",
    "        stemmed = stemmer.stem(WordNetLemmatizer().lemmatize(token, pos='v'))\n",
    "    else:\n",
    "        stemmed = ''\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic analysis\n",
    "\n",
    "Below is the code which performs topic analysis. I only attempt to divine the topics of clusters that have at least 10 documents. For each cluster, I create the bag of words for each document. I then use the gensim library's LDA function to find the topic that best defines the cluster.\n",
    "\n",
    "This code is largely inspired by a tutorial on Latent Dirichlet Allocation (https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 1 \n",
      "Words: 0.015*\"imag\" + 0.007*\"data\" + 0.006*\"graphic\" + 0.005*\"program\" + 0.005*\"file\" + 0.005*\"avail\" + 0.004*\"format\" + 0.004*\"line\" + 0.004*\"packag\" + 0.004*\"display\"\n",
      "Topic: 2 \n",
      "Words: 0.007*\"write\" + 0.007*\"peopl\" + 0.006*\"articl\" + 0.005*\"know\" + 0.004*\"weapon\" + 0.004*\"gun\" + 0.004*\"right\" + 0.004*\"think\" + 0.004*\"like\" + 0.003*\"govern\"\n",
      "Topic: 3 \n",
      "Words: 0.009*\"write\" + 0.008*\"stephanopoulo\" + 0.007*\"think\" + 0.007*\"articl\" + 0.007*\"say\" + 0.006*\"peopl\" + 0.006*\"presid\" + 0.005*\"know\" + 0.004*\"govern\" + 0.004*\"go\"\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "\n",
    "for cluster, files in cluster_to_file.items():\n",
    "    \n",
    "    # get files from clusters w more than 10 entries\n",
    "    if len(files) < 10:\n",
    "        continue\n",
    "    corpus = [cleaned_sentences[file] for file in files]\n",
    "    \n",
    "    # process files in cluster\n",
    "    stemmed_sentences = []\n",
    "    for words in corpus:\n",
    "        words = words.split()\n",
    "        new_sentence = ' '.join([process_tokens(word) for word in words]).split()\n",
    "        stemmed_sentences.append(new_sentence)\n",
    "        \n",
    "    # perform lda analysis by building idx dictionary for each word, creating bag of words model\n",
    "    # and training the lda model\n",
    "    dictionary = gensim.corpora.Dictionary(stemmed_sentences)\n",
    "    new_corpus = [dictionary.doc2bow(doc) for doc in stemmed_sentences]\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(new_corpus, num_topics=1, id2word=dictionary, passes=5)\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print('Topic: {} \\nWords: {}'.format(n, topic))\n",
    "        n+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "It seems that some of the clusters are very distinct, while others are very similar. Notably, one cluster seems to be about computers, while the others seem to be about politics. There is notable overlap in the 'political' clusters, with both groups being defined by words like 'govern'. However, the differences seem to imply each cluster represents a different political topic -- such as gun rights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Cluster visualisation\n",
    "\n",
    "To perform cluster visualisation, I apply T-SNE to the encoded documents. This projects the encodings into two-dimensional space, where they can be plotted. I colour each point by the cluster group it is a member of, or I colour it black if it is not a member of a large cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "files = new_files\n",
    "\n",
    "two_dim_rep = TSNE().fit_transform(points)\n",
    "x, y = zip(*list(two_dim_rep))\n",
    "\n",
    "colours= ['r','g','b','c','y']\n",
    "\n",
    "col = 0\n",
    "\n",
    "for cluster, clustered_files in cluster_to_file.items():\n",
    "    idxs = []\n",
    "    for file in clustered_files:\n",
    "        idx = files.index(file)\n",
    "        idxs.append(idx)\n",
    "    xs = [x[i] for i in idxs]\n",
    "    ys = [y[i] for i in idxs]\n",
    "    if len(clustered_files) < 4:\n",
    "        plt.scatter(xs, ys, c = 'black')\n",
    "    else:\n",
    "        plt.scatter(xs, ys, c = colours[col])\n",
    "        col+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "This supports the analysis suggested by the topic analysis. As can be seen on the scatterplot, one cluster (that relating to computer graphics) is very separate from the other, politics based clusters. Indeed, although the politics clusters do appear to have some degree of separation, they clearly overlap. Whether this suggests that the dataset is characterised by 2 clusters rather than 3 is a somewhat subjective distinction. What one can say through the analysis conducted so far is that there are *at least* two document clusters in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "Below are some alternative methods I used for encoding the sentences. By running the below code, and changing the size of the dimension variable in Section 2, the code should be able to be run as normal with these changed encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra\n",
    "\n",
    "I tried BERT encoding the body text of the documents. This actually produced worse results, but if this is something you would like to try using my code, I used BERT-as-a-service to encode the documents. The below code encodes the sentences using this methodology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = bc.encode(cleaned_sentences_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra\n",
    "\n",
    "I also encoded the sentences by taking the average of all the GloVe word embeddings. Below is the code to do this. I used GloVe with 100 dimension word vectors trained on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "points = []\n",
    "vecs = np.zeros((400000, 100), dtype=np.float32)\n",
    "\n",
    "with open('../glove.6b/glove.6B.100d.txt','r',encoding='utf8') as dictionary:\n",
    "    for i, line in enumerate(dictionary):\n",
    "\n",
    "        split = line.split()\n",
    "        word = split[0]\n",
    "        vector = np.asarray(split[1:], \"float32\")\n",
    "        glove[word] = vector\n",
    "        vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)\n",
    "\n",
    "glove['<UNK>'] = np.mean(vecs, axis=0)\n",
    "\n",
    "# part of above solution taken from https://stackoverflow.com/questions/49239941/what-is-unk-in-the-pretrained-glove-vector-files-e-g-glove-6b-50d-txt\n",
    "\n",
    "for file in cleaned_sentences.keys():\n",
    "    words = cleaned_sentences[file]\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            word_vectors.append(glove[word])\n",
    "        except KeyError:\n",
    "            word_vectors.append(glove['<UNK>'])\n",
    "    word_vectors = np.asarray(word_vectors)\n",
    "    tf_idf = np.asarray([word for word in words])\n",
    "    points.append(np.average(word_vectors, axis = 0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
